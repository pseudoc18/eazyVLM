<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs">
  <meta name="keywords" content="Hallucination Detection and Mitigation in LVLMs">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> Hallucinatory Image Tokens: A Training-free <span style="font-family: 'Castoro', serif; font-style: italic;">EAZY</span> Approach on Detecting and Mitigating Object Hallucinations in LVLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Hallucinatory Image Tokens: A Training-free <span style="font-family: 'Castoro', serif; font-style: italic;">EAZY</span> Approach on Detecting and Mitigating Object Hallucinations in LVLMs</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/liweiche/">Liwei Che</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Tony Qingze Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Jing Jia</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Weiyi Qin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ruixiangtang.net/">Ruixiang Tang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://seqam.rutgers.edu/">Vladimir Pavlovic</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Rutgers University</span>
          </div>

          <div class="has-text-centered">
            <img src="./static/images/Rutgers.png" alt="Rutgers Logo" style="margin-top: 10px; width: 150px; height: auto;">
          </div>

          <div class="has-text-centered">
            <p style="font-weight: bold; font-size: 1.5rem;">ICCV2025</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.07772"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.07772"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section" style="display: flex; flex-direction: column; align-items: center; text-align: center;">
  <div class="container is-max-desktop" style="width: 100%;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Despite their remarkable potential, Large Vision-Language Models (LVLMs) still face challenges with object hallucination, a problem where their generated outputs mistakenly incorporate objects that do not actually exist. Although most works focus on addressing this issue within the language-model backbone, our work shifts the focus to the image input source, investigating how specific image tokens contribute to hallucinations. Our analysis reveals a striking finding: a small subset of image tokens with high attention scores are the primary drivers of object hallucination. By removing these hallucinatory image tokens (only 1.5% of all image tokens), the issue can be effectively mitigated. This finding holds consistently across different models and datasets. Building on this insight, we introduce EAZY, a novel, training-free method that automatically identifies and Eliminates hAllucinations by Zeroing out hallucinatorY image tokens. We utilize EAZY for unsupervised object hallucination detection, achieving 15% improvement compared to previous methods. Additionally, EAZY demonstrates remarkable effectiveness in mitigating hallucinations while preserving model utility and seamlessly adapting to various LVLM architectures.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered" style="margin-top: 40px;">
      <div class="column" style="width: 100%; text-align: center;">
        <img src="./static/images/zero_out.png"
             alt="Main Figure"
             style="width: 400%; height: auto;">
      </div>
    </div>

    <div class="columns is-left has-text-centered" style="margin-top: 20px;">
      <div class="column is-full" style="max-width: 80%; padding: 0;">
        <p style="font-size: 0.9rem; margin-top: 10px; font-style: italic; color: #666;">
          Figure 1: Removing three image tokens results in the elimination of the hallucinated objects, "apples" and "oranges", and reveals the real object "kiwis".
        </p>
        <p style="text-align: justify; font-size: 1rem; line-height: 1.5; margin: 0;">
          EAZY identifies and removes hallucinatory image tokens from the input image, effectively mitigating hallucinations in LVLMs. The method is training-free and can be applied to various LVLM architectures. In this example, EAZY removes three image tokens, which eliminates the hallucinated objects, "apples" and "oranges", revealing the real object "kiwis".
        </p>
      </div>
    </div>

  </div>
</section>




<section class="section" style="display: flex; flex-direction: column; align-items: center; text-align: center;">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Connecting Generated Object Tokens with Their Visual Anchor</h2>
    <div class="content has-text-justified">
      <p>
        We first investigated an important question: How do Large Vision-Language Models (LVLMs) extract visual information and generate text tokens accordingly? 
      </p>
    </div>

    <div class="columns is-vcentered" style="margin-top: 30px;">
      <div class="column is-full">
        <div class="has-text-centered">
          <img src="./static/images/layer_attention.png"
               alt="token-wise attention heatmap"
               style="width: 200%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
          <p class="is-size-7 has-text-grey" style="margin-top: 10px;">
            Figure 2: Token-wise attention heatmap on Layer-32 and Layer-15 of LLaVA-1.5
          </p>
        </div>
      </div>
    </div>
 

  <div class="columns is-vcentered" style="margin-top: 30px;">
    <div class="column is-full">
      <h3 class="title is-4">Locating Object Information in Model Layers.</h3>
      <div class="content has-text-justified">
        <p>
          Through a comprehensive analysis of attention patterns, we found that Large Vision-Language Models (LVLMs) primarily extract and process object-related information in their middle-to-late layers, not in the initial or final layers as one might expect. We introduced a new metric, the Object Localization Visual Attention Ratio (OL-VAR), to quantify this. The results show that for a model like LLaVA-1.5, layers 10 through 25 are critical for linking generated object tokens to their corresponding regions in the image. This discovery provides a precise tool to understand where the model "looks" when generating text about objects.
        </p>
      </div>
    </div>
  </div>
  <div class="columns is-vcentered" style="margin-top: 30px;">
    <div class="column is-full">
      <div class="has-text-centered">
        <h4 class="title is-5">OL-VAR</h4>
        <img src="./static/images/olvar_llava1.5.png"
             alt="OL-VAR"
             style="width: 40%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
      </div>
    </div>
  </div>
  <div class="columns is-centered">
    <div class="column is-full">
      <p class="is-size-7 has-text-grey has-text-centered" style="margin-top: 10px;">
        Figure3: Visualization showing OL-VAR scores peaking in middle layers and highest at Layer-15, which is aligned with our observation of the token-wise attention heatmap pattern in Figure 2. 
      </p>
    </div>
  </div>


  <div class="columns is-vcentered" style="margin-top: 30px;">
    <div class="column is-full">
      <div class="has-text-centered">
        <h4 class="title is-5">Dog to Image Token Attention Heatmap</h4>
        <img src="./static/images/dog heatmap_bbox.png"
             alt="dog to image token attention heatmap"
             style="width: 40%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
      </div>
    </div>
  </div>
  <div class="columns is-centered">
    <div class="column is-full">
      <p class="is-size-7 has-text-grey has-text-centered" style="margin-top: 10px;">
        Figure4: Utilizing the layer-15 attention, we can effectively bound the corresponding image region of "dog".
      </p>
    </div>
  </div>
  </div>
</section>



<section class="section" style="display: flex; flex-direction: column; align-items: center; text-align: center;">
  <div class="container is-max-desktop" style="width: 100%;">
    <h2 class="title is-3" style="margin-top: 40px;">Methodology</h2>

    <div class="columns is-centered" style="margin-top: 20px;">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Hallucination Detection</h3>
        <div class="content has-text-justified">
          <p>
            The EAZY method identifies object hallucinations by leveraging the insight that real and hallucinated objects respond differently to the removal of high-attention image tokens. The process begins by generating an initial response from the LVLM for a given image. From this response, potential object tokens are extracted using NLP techniques like POS tagging. For each identified object token, the method pinpoints the top-K image tokens that received the highest attention scores from it. These top-K tokens are considered candidate Hallucinatory Image Tokens (HITs).
          </p>
          <p>
            Next, the model's input is modified by "zeroing out" these candidate HITs, replacing their embeddings with zero vectors. A new response is then generated using this modified input. If an object token from the original response disappears in the new response, EAZY classifies it as a hallucination. Conversely, if the object token remains, it is classified as a real object. This differential behavior serves as a robust, training-free mechanism for detecting hallucinations, showing that real objects are largely unaffected by this zeroing-out process, while hallucinated ones are effectively eliminated.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered" style="margin-top: 20px;">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Hallucination Mitigation</h3>
        <div class="content has-text-justified">
          <p>
            The mitigation process in EAZY follows a similar logic to detection and is designed to correct the initial, potentially flawed output of an LVLM. First, an initial inference is performed to get the model's description of the image. All object tokens, both real and hallucinated, are extracted from this initial response. For all identified objects, the method aggregates the top-K candidate HITs into a single set.
          </p>
          <p>
            These aggregated candidate HITs are then zeroed out from the image token sequence to perform a second, "estimation" inference. By comparing the initial response with the second one, EAZY identifies the objects that disappeared, flagging them as hallucinations. Finally, the candidate HITs corresponding only to these confirmed hallucinated objects are collected to form a final zero-out list. This final set of tokens is zeroed out from the original input, and a final inference is run to produce a corrected, mitigated response that is free of the identified hallucinations while preserving the accurately described objects.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered" style="margin-top: 40px;">
      <div class="column" style="width: 100%; text-align: center;">
        <img src="./static/images/EAZY.png"
             alt="Method Overview"
             style="width: 100%; height: auto;">
      </div>
    </div>

    <div class="columns is-centered has-text-left" style="margin-top: 10px;">
      <div class="column is-full" style="max-width: 100%; padding: 0 10%;">
        <p style="font-size: 0.9rem; margin-top: 10px; font-style: italic; color: #666;">
          Figure 6: Overview of the proposed EAZY method. The process starts with an image input encoded into image tokens. The LVLM generates an initial response with hallucinated objects (apples, oranges). EAZY estimates HITs via text-to-image token-wise attention distribution. With HITs zeroed out, the final response has the hallucinations disappeared, revealing correct objects (kiwis).
        </p>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title">Evaluation</h2>
    <div class="content has-text-justified">
      <p>
        We conduct extensive experiments to evaluate the effectiveness of EAZY in detecting and mitigating object hallucinations across various LVLMs and datasets. Our results demonstrate that EAZY significantly outperforms existing methods in both detection accuracy and mitigation effectiveness, while maintaining model utility.
      </p>
    </div>

    <div style="margin-top: 30px;">
      
      <div class="columns is-centered" style="margin-top: 20px;">
        <div class="column is-two-thirds">
            <div class="has-text-centered">
              <img src="./static/images/chair_exp.png"
                   alt="CHAIR Results"
                   style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
              <p class="has-text-weight-semibold" style="margin-top: 15px;">CHAIR Results</p>
              <p class="is-size-7 has-text-grey">
                EAZY's validation results show a significant reduction in hallucination rates across multiple LVLMs, demonstrating its robustness and adaptability.
              </p>
            </div>
        </div>
      </div>


      <div class="columns is-centered" style="margin-top: 30px;">
        <div class="column is-two-thirds">
            <div class="has-text-centered">
              <img src="./static/images/pope_exp.png"
                   alt="POPE Results"
                   style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
              <p class="has-text-weight-semibold" style="margin-top: 15px;">POPE Results</p>
              <p class="is-size-7 has-text-grey">
                Evaluation Result on POPE. We take the average accuracy and F1 score of random, popular, and adversarial models.
              </p>
            </div>
        </div>
      </div>

      <div class="columns is-centered" style="margin-top: 30px;">
        <div class="column is-two-thirds">
            <div class="has-text-centered">
              <img src="./static/images/detection_curve.png"
                   alt="POPE Results"
                   style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
              <p class="has-text-weight-semibold" style="margin-top: 15px;">Hallucination Detection Curves Comparison</p>
              <p class="is-size-7 has-text-grey">
                Object Hallucination Detection Curves on Hall-COCO. We present the Precision-Recall and ROC curves of the proposed OH detection method and baselines.
              </p>
            </div>
        </div>
      </div>


      <div class="columns is-centered" style="margin-top: 30px;">
        <div class="column is-two-thirds">
            <div class="has-text-centered">
              <img src="./static/images/detection_exp.png"
                   alt="Detection Performance"
                   style="width: 60%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
              <p class="has-text-weight-semibold" style="margin-top: 15px;">Detection Performance</p>
              <p class="is-size-7 has-text-grey">
                OH Detection Results on Hall-COCO. PR(RO) represents the precision of real objects (positive instances), while PR(OH) represents the precision of object hallucination (negative instances).
              </p>
            </div>
        </div>
      </div>

      <div class="columns is-centered" style="margin-top: 30px;">
        <div class="column is-two-thirds">
            <div class="has-text-centered">
              <img src="./static/images/MLLM_result.png"
                   alt="Detection Performance"
                   style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
              <p class="has-text-weight-semibold" style="margin-top: 15px;">Evaluation on MLLM Benchmarks</p>
              <p class="is-size-7 has-text-grey">
                EAZY maintains the general capabilities of LVLMs.
              </p>
            </div>
        </div>
      </div>


    </div>

    <div class="content has-text-justified" style="margin-top: 40px;">
      <h3 class="title is-4">Key Results</h3>
      <ul>
        <li><strong>Detection Accuracy:</strong> EAZY achieves 15% improvement in unsupervised object hallucination detection compared to previous methods.</li>
        <li><strong>Mitigation Effectiveness:</strong> By removing only 1.5% of image tokens, EAZY effectively mitigates hallucinations while preserving model utility.</li>
        <li><strong>Cross-Model Generalization:</strong> The method demonstrates consistent performance across various LVLM architectures including LLaVA, Shikra, and LLaVA-Next.</li>
        <li><strong>Training-Free Approach:</strong> EAZY requires no additional training, making it easily adaptable to existing models.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title">Case Studies</h2>
    <div class="content has-text-justified">
      <p>
        We present qualitative case studies demonstrating EAZY's effectiveness in detecting and mitigating object hallucinations across diverse scenarios. These examples showcase how our method identifies problematic image tokens and successfully eliminates hallucinated objects while preserving the accuracy of real object detection.
      </p>
      </div>
          <div id="results-carousel" class="carousel results-carousel" style="width: 100%;">
            <div class="item" style="text-align: center;">
              <img src="static/images/cat_case.png" alt="Cat Case Study" style="width: 80%; height: auto;">
            </div>
            <div class="item" style="text-align: center;">
              <img src="static/images/airport_case.png" alt="Airport Case Study" style="width: 80%; height: auto;">
            </div>
            <div class="item" style="text-align: center;">
              <img src="static/images/snow_case.png" alt="Snow Case Study" style="width: 80%; height: auto;">
            </div>
            <div class="item" style="text-align: center;">
              <img src="static/images/night_case.png" alt="Night Case Study" style="width: 80%; height: auto;">
            </div>
            <div class="item" style="text-align: center;">
              <img src="static/images/street_case.png" alt="Street Case Study" style="width: 80%; height: auto;">
            </div>
          </div>
        </div>

    </div>


    <div class="content has-text-justified" style="margin-top: 40px;">
      <h3 class="title is-4">Case Study Insights</h3>
      <ul>
        <li><strong>Diverse Scenarios:</strong> EAZY performs consistently across various scene types including indoor, outdoor, and complex environments.</li>
        <li><strong>Precision:</strong> The method accurately identifies hallucinatory tokens without affecting genuine object detection.</li>
        <li><strong>Robustness:</strong> Demonstrates effectiveness across different object categories and scene complexities.</li>
        <li><strong>Visual Quality:</strong> Maintains image quality and context while removing problematic tokens.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{che2025hallucinatoryimagetokenstrainingfree,
      title={Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs}, 
      author={Liwei Che and Tony Qingze Liu and Jing Jia and Weiyi Qin and Ruixiang Tang and Vladimir Pavlovic},
      year={2025},
      eprint={2503.07772},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.07772}, 
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2503.07772">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website is borrowed from the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, we thank the author for his contribution.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
